{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":3136,"databundleVersionId":26502,"sourceType":"competition"}],"dockerImageVersionId":30626,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/ramirazodi/titanic-flex-vote-pipe?scriptVersionId=157857665\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"markdown","source":"# Highly Flexible and Nested Ensemble Pipelines Leading into a Voting Regressor & All Steps & Parameters Optimized through GridSearchCV","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport os\nimport re\nimport sklearn\nimport matplotlib\nimport warnings\nfrom sklearn import set_config\nfrom sklearn.base import BaseEstimator, TransformerMixin\nfrom sklearn.compose import TransformedTargetRegressor\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.compose import ColumnTransformer, make_column_selector, make_column_transformer\nfrom sklearn.pipeline import Pipeline, make_pipeline\nfrom sklearn.model_selection import train_test_split, cross_val_predict\nfrom sklearn.preprocessing import OneHotEncoder, OrdinalEncoder, StandardScaler, MinMaxScaler, RobustScaler, PolynomialFeatures \nfrom sklearn.preprocessing import PowerTransformer, SplineTransformer, KBinsDiscretizer, FunctionTransformer, QuantileTransformer\nfrom sklearn.feature_selection import VarianceThreshold, SelectKBest, SelectPercentile, f_regression, f_classif, SelectFromModel\nfrom sklearn.decomposition import PCA\nfrom sklearn.linear_model import LogisticRegression, SGDClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.svm import SVC\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.metrics import make_scorer, accuracy_score, confusion_matrix, classification_report\nfrom sklearn.model_selection import cross_val_score, StratifiedKFold\nfrom sklearn.ensemble import VotingClassifier, StackingClassifier, RandomForestClassifier, GradientBoostingClassifier, ExtraTreesClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom IPython.display import display, HTML\nfrom catboost import CatBoostRegressor\nimport lightgbm as lgb\nimport xgboost as xgb","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Utilize set_output API of scikit to maintain padnas dataframe capabiity throughout the pipelines\nset_config(display='diagram', transform_output=\"pandas\")","metadata":{"execution":{"iopub.status.busy":"2024-01-05T21:30:54.291947Z","iopub.execute_input":"2024-01-05T21:30:54.292935Z","iopub.status.idle":"2024-01-05T21:30:54.300005Z","shell.execute_reply.started":"2024-01-05T21:30:54.292873Z","shell.execute_reply":"2024-01-05T21:30:54.298386Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\ndf_raw = pd.read_csv('/kaggle/input/titanic/train.csv')","metadata":{"execution":{"iopub.status.busy":"2024-01-05T21:30:54.301941Z","iopub.execute_input":"2024-01-05T21:30:54.302458Z","iopub.status.idle":"2024-01-05T21:30:54.335914Z","shell.execute_reply.started":"2024-01-05T21:30:54.30241Z","shell.execute_reply":"2024-01-05T21:30:54.333936Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Quick EDA","metadata":{}},{"cell_type":"code","source":"# Mapping dictionary to make exctracted titles consistent\ntitle_map = {\n    \"Master\": \"Master\", \"Mr\": \"Mr\",\n    \"Mlle\": \"Miss\", \"Ms\": \"Miss\", \"Mme\": \"Mrs\",\n    \"Dr\": \"Professional\", \"Rev\": \"Professional\",\n    \"Major\": \"Military\", \"Col\": \"Military\", \"Capt\": \"Military\",\n    \"Jonkheer\": \"Nobility\", \"Don\": \"Nobility\", \"Sir\": \"Nobility\", \"Lady\": \"Nobility\", \"Countess\": \"Nobility\"\n}\n\n# Ticket prefix mapping to handle complex variations\nticketprefix_mapping = {\n    'STONO': 'STON/O', 'STONO': 'STON/O', 'STONO2': 'STON/O',\n    'SOTONOQ': 'SOTON/OQ' ,'SOTONOQ': 'SOTON/OQ','SOTON/O.Q': 'SOTON/O','SOTON/OQ': 'SOTON/O',\n    'SCPARIS': 'SC/PARIS','SCPARIS': 'SC/PARIS', 'SC/PARIS': 'SC/PARIS', 'S.C./PARIS': 'SC/PARIS',\n    'C.A.': 'CA', 'C.A': 'CA', 'CA.': 'CA',\n    'SCAH': 'SC/AH',\n    'W./C': 'W/C', 'W.C': 'W/C',\n    'S.W./PP' : 'SW/PP',\n    'W.E.P': 'WE/P',\n    'S.O./P.P': 'SO/PP', 'S.O.P': 'SO/PP',\n    'P/PP': 'PP',\n    #'WE/P': 'W/C',\n    #'W.E.P': 'W/C',\n    #'S.O.C': 'SO/C',\n    #'SC/AH': 'SC/PARIS',\n}\n\n\n# Define Age imputation function\ndef age_impute(df):\n    \n    age_means = df.groupby(['Title', 'TicketGroupSize', 'Parch', 'SibSp'])['Age'].transform('mean')\n\n    if age_means.isnull().any():\n        age_means = age_means.fillna(df.groupby(['Title', 'TicketGroupSize'])['Age'].transform('mean'))\n        \n    if age_means.isnull().any():\n        age_means = age_means.fillna(df.groupby(['Title'])['Age'].transform('mean'))\n    \n    # Fallback\n    overall_mean = df['Age'].mean()\n        \n    df['Age_mean'] = df['Age'].fillna(age_means).fillna(overall_mean)\n    \n    return df\n\n# Define function that uses Catboost to impute Age column. The Catboost model in this function has already been hyper-parameter optimized on this dataset\ndef cb_age_impute(df, target='Age', features=['Pclass', 'SibSp', 'Parch', 'Title', 'Sex']):\n\n    df['Age_cb'] = df[target]\n    \n    # Splitting the data into training and test sets based on original 'Age'\n    df_train = df.dropna(subset=[target])\n    df_test = df[df[target].isnull()]\n    \n    X_train = df_train[features]\n    y_train = df_train[target]\n    X_test = df_test[features]\n\n    # Define and fit the model\n    cb_model = CatBoostRegressor(iterations=100,\n                                 learning_rate=0.05,\n                                 depth=6,\n                                 cat_features=['Title', 'Sex'],\n                                 loss_function='MAE',\n                                 random_seed=42,  \n                                 verbose=False)\n\n    cb_model.fit(X_train, y_train)\n    \n    # Predict 'Age' for the test data\n    predicted_ages = cb_model.predict(X_test)\n\n    # Replace the NaNs in 'Age_cb' with the predictions\n    df.loc[df[target].isnull(), 'Age_cb'] = predicted_ages\n\n    return df\n\n\n\n# Define TicketPrefix sparcity reducer.The function groups together TicketPrefix values that have less than cutoff number of observation  \ndef group_rare_prefixes(df, column='TicketPrefix', cutoff=5):\n    \n    prefixes_to_group = df[column].value_counts()\n    rare_prefixes = prefixes_to_group[prefixes_to_group < cutoff].index\n    \n    # Create a mask where True corresponds to values to be grouped into \"OTHER\"\n    is_rare = df[column].isin(rare_prefixes)\n    \n    # Use the mask to assign \"OTHER\" to the rare values\n    df[column] = df[column].where(~is_rare, other=\"OTHER\")\n    return df\n\n\ndef deck_impute(df, level=7):\n    # Capture original missing Deck values\n    original_missing_deck = df['Deck'].isnull().astype(int)\n\n    # Create mode mappings for Deck based on different groupings, now including 'Embarked' and 'Pclass'\n    modes = {\n        1: df.groupby(['LastName', 'Ticket', 'Embarked', 'Pclass', 'GroupSize', 'LastNameRepeat'])['Deck'],\n        2: df.groupby(['LastName', 'TicketPrefix', 'Embarked', 'Pclass', 'GroupSize', 'LastNameRepeat'])['Deck'],\n        3: df.groupby(['LastName', 'Embarked', 'Pclass', 'GroupSize', 'LastNameRepeat'])['Deck'],\n        4: df.groupby(['TicketPrefix', 'Title', 'FareQcut', 'Embarked', 'Pclass', 'GroupSize', 'LastNameRepeat'])['Deck'],\n        5: df.groupby(['FareQcut', 'Title', 'Embarked', 'Pclass', 'GroupSize', 'LastNameRepeat'])['Deck'],\n        6: df.groupby(['FareQcut', 'Title', 'Pclass', 'LastNameRepeat'])['Deck'],\n        7: df.groupby(['FareQcut', 'Title'])['Deck'],\n    }\n\n    df_imputed = df.copy()\n\n    # Loop through the modes and merge them if they're at or below the specified level\n    for i in range(1, level+1):\n        mode_mapping = modes[i].apply(lambda x: x.mode()[0] if not x.mode().empty else np.nan).rename(f'DeckMode{i}')\n        keys = modes[i].grouper.names  # extract grouping columns for the merge\n        df_imputed = df_imputed.merge(mode_mapping, left_on=keys, right_index=True, how='left')\n\n    # Assign Deck values conditionally based on level and drop temporary DeckMode columns\n    for i in range(1, level+1):\n        df_imputed = df_imputed.assign(Deck=lambda df_: np.where(df_['Deck'].isnull(), df_[f'DeckMode{i}'], df_['Deck']))\n        df_imputed = df_imputed.drop(columns=f'DeckMode{i}')\n\n    # Final assignment for Deck and Deck_Missing\n    df_imputed = df_imputed.assign(\n        Deck=lambda df_: df_['Deck'].fillna('Unknown'),\n        Deck_Missing=original_missing_deck\n    )\n\n    return df_imputed","metadata":{"execution":{"iopub.status.busy":"2024-01-05T21:30:54.338443Z","iopub.execute_input":"2024-01-05T21:30:54.338943Z","iopub.status.idle":"2024-01-05T21:30:54.375084Z","shell.execute_reply.started":"2024-01-05T21:30:54.338904Z","shell.execute_reply":"2024-01-05T21:30:54.373254Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Create copy of raw data for EDA purposes to avoid contamination \ndf_eda = (df_raw\n          .copy()\n          .assign(\n              GroupSize=lambda df_: df_.SibSp.add(df_.Parch),\n              Deck=lambda df_: df_.Cabin.str.extract('([A-Za-z])', expand=False),\n              NumCabins=lambda df_: df_.Cabin.str.count(' ').add(1),\n              FirstCabNum=lambda df_: df_.Cabin.str.extract('(\\d+)', expand=False),\n              TicketPrefix=lambda df_: df_.Ticket.str.extract('([A-Za-z./]*[A-Za-z])', expand=False).str.replace('\\.', '').str.upper(),\n              TicketNum=lambda df_: df_.Ticket.str.extract('(\\d+)$'),\n              TicketGroupSize=lambda df_: df_.Ticket.map(df_.Ticket.value_counts()).astype(int),\n              TicketNumDigits=lambda df_: df_.TicketNum.str.len(),\n              Title=lambda df_: df_.Name.str.extract(' ([A-Za-z]+)\\.', expand=False),\n              LastName=lambda df_: df_.Name.str.extract('^([^,]+),', expand=False),\n              LastNameRepeat=lambda df_: df_.LastName.map(df_.LastName.value_counts()).astype(int),\n              FareQcut=lambda df_: pd.qcut(df_.Fare, q=10, labels=[bin_label for bin_label in range(1, 11)]).astype(int),\n          )\n          .assign(\n              Title=lambda df_: df_.Title.map(title_map).fillna(df_.Title),\n              TicketPrefix=lambda df_: df_.TicketPrefix.map(ticketprefix_mapping).fillna(df_.TicketPrefix).fillna('No_Prefix'),\n              NumCabins=lambda df_: df_.NumCabins.fillna(df_.NumCabins.mode()[0]),\n              TicketNumDigits=lambda df_: df_.TicketNumDigits.fillna(df_.TicketNumDigits.fillna(0)).astype(int),\n              TicketNum=lambda df_: df_.TicketNum.fillna(0).astype(int),\n              Embarked=lambda df_: df_.Embarked.fillna(df_.Embarked.mode()[0])\n          )\n          .pipe(group_rare_prefixes, column='TicketPrefix', cutoff=5)\n          .pipe(cb_age_impute, target='Age', features=['Pclass', 'SibSp', 'Parch', 'Title', 'Sex'])\n          .pipe(deck_impute, level=5)\n          .drop(columns=['Name', 'Cabin', 'Ticket', 'FirstCabNum'])\n          .assign(\n              Sex_fctr=lambda df_: pd.factorize(df_.Sex)[0],\n              Embarked_fctr=lambda df_: pd.factorize(df_.Embarked)[0],\n              Deck_fctr=lambda df_: pd.factorize(df_.Deck)[0],\n              TicketPrefix_fctr=lambda df_: pd.factorize(df_.TicketPrefix)[0],\n              Title_fctr=lambda df_: pd.factorize(df_.Title)[0],\n          )\n         )\n\ndisplay(df_raw.columns)\ndisplay(df_eda.shape)\ndisplay(df_eda.info())\ndisplay(df_eda.describe().round(1))\ndisplay(df_eda.tail(50))","metadata":{"execution":{"iopub.status.busy":"2024-01-05T21:30:54.381057Z","iopub.execute_input":"2024-01-05T21:30:54.38168Z","iopub.status.idle":"2024-01-05T21:30:55.582067Z","shell.execute_reply.started":"2024-01-05T21:30:54.381627Z","shell.execute_reply":"2024-01-05T21:30:55.580821Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Correlation Matrix\nplt.figure(figsize=(12, 8)) \n\ncmap = sns.cubehelix_palette(start=0.55, rot=-.8, dark=-.0, light=.7, reverse=False, as_cmap=True)\n\n# Generate a heatmap\nsns.heatmap(\n    data = df_eda.select_dtypes(include=[np.number]).corr(),\n    annot=True,          \n    fmt=\".1f\",          \n    cmap='viridis',  \n    annot_kws={\"size\": 8} \n)\n\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-01-05T21:30:55.583931Z","iopub.execute_input":"2024-01-05T21:30:55.584722Z","iopub.status.idle":"2024-01-05T21:30:57.729997Z","shell.execute_reply.started":"2024-01-05T21:30:55.584673Z","shell.execute_reply":"2024-01-05T21:30:57.728483Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Visualization of EDA","metadata":{}},{"cell_type":"code","source":"sns.countplot(data=df_eda, x=\"Survived\")\n\n# Customize these variables to adjust the appearance of all plots\nheight = 4\naspect = 2.5\npalette = 'viridis'\nbinwidth_age = 5\nbinwidth_group = 1\nage_range = (0, 100)\ngroup_range = (1, 8)\n\n# Define the size for the FacetGrid plots to match catplots\nfg_height = height * 0.75\nfg_width = height * aspect \nfg_asp_adj = 0.75\n\n# Define a dictionary of plots for easy management\nplots = {\n    'Fare_Sex_Survived': sns.catplot(\n        data=df_eda, x=\"Fare\", y=\"Sex\", hue='Survived', kind=\"boxen\",\n        height=height, aspect=aspect),\n\n    'Age_Sex_Survived': sns.catplot(\n        data=df_eda, x=\"Age\", y=\"Sex\", hue='Survived', kind=\"boxen\",\n        height=height, aspect=aspect),\n\n    'Fare_Embarked_Survived': sns.catplot(\n        data=df_eda, x=\"Fare\", y=\"Embarked\", hue='Survived', kind=\"boxen\",\n        height=height, aspect=aspect),\n\n    'Fare_Title_Survived_Bar': sns.catplot(\n        data=df_eda, x=\"Fare\", y=\"Title\", hue='Survived', kind=\"bar\", estimator='mean',\n        height=height, aspect=aspect),\n\n    'Age_Pclass_Survived_Hist': sns.FacetGrid(\n        df_eda, col=\"Survived\",  row=\"Pclass\", palette=palette,\n        height=fg_height, aspect=aspect / fg_asp_adj, margin_titles=True, despine=False\n    ).map_dataframe(sns.histplot, x=\"Age\", binwidth=binwidth_age, binrange=age_range),\n\n    'TicketGroupSize_Pclass_Survived_Hist': sns.FacetGrid(\n        df_eda, col=\"Survived\",  row=\"Pclass\", palette=palette,\n        height=fg_height, aspect=aspect / fg_asp_adj, margin_titles=True, despine=False\n    ).map_dataframe(sns.histplot, x=\"TicketGroupSize\", binwidth=binwidth_group, binrange=group_range),\n\n    'Age_Embarked_Survived_Hist': sns.FacetGrid(\n        df_eda, col=\"Survived\",  row=\"Embarked\", palette=palette,\n        height=fg_height, aspect=aspect / fg_asp_adj, margin_titles=True, despine=False\n    ).map_dataframe(sns.histplot, x=\"Age\", binwidth=binwidth_age, binrange=age_range),\n\n    # This plot is explicitly separated to avoid overlap\n    'Pclass_Deck_Count': {\n        'type': 'countplot',\n        'data': df_eda,\n        'x': \"Pclass\",\n        'hue': \"Deck\"\n    }\n}\n\n\n# Display all plots\nfor plot_name, plot_obj in plots.items():\n    plt.figure()  # This ensures each plot is on a separate figure\n    print(f\"Displaying: {plot_name}\")\n    plot_obj\n    plt.show()\n\nsns.countplot(data=df_eda, x=\"Pclass\", hue=\"Deck\")","metadata":{"execution":{"iopub.status.busy":"2024-01-05T21:30:57.731658Z","iopub.execute_input":"2024-01-05T21:30:57.732153Z","iopub.status.idle":"2024-01-05T21:31:09.24072Z","shell.execute_reply.started":"2024-01-05T21:30:57.732108Z","shell.execute_reply":"2024-01-05T21:31:09.239283Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.countplot(data=df_eda, x=\"Pclass\", hue=\"NumCabins\")","metadata":{"execution":{"iopub.status.busy":"2024-01-05T21:31:09.24304Z","iopub.execute_input":"2024-01-05T21:31:09.243524Z","iopub.status.idle":"2024-01-05T21:31:09.59226Z","shell.execute_reply.started":"2024-01-05T21:31:09.243479Z","shell.execute_reply":"2024-01-05T21:31:09.590772Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Test (Hold-Out) / Train Split","metadata":{}},{"cell_type":"code","source":"df = df_raw.copy()\n\nfeature_col_names = df.columns.drop('Survived')\ntarget = df.loc[:, 'Survived']\nfeatures_all = df.loc[:, feature_col_names]\n\nX_train, X_hold, y_train, y_hold = train_test_split(\n    features_all,\n    target,\n    random_state=1,\n    shuffle=True,\n    #stratify=target,       # Dataset seems imbalanced and requires stratification\n    test_size=0.001,        # Set to 0.001 after optimization using test_size=0.20 to get gridsearch to train on full data prior to predicting. Revert back to test_size=0.20 as needed!\n)\n\ndisplay(f'X_train Shape: {X_train.shape}   y_train Shape: {y_train.shape}')\ndisplay(f'X_hold Shape: {X_hold.shape}   y_hold Shape: {y_hold.shape}')","metadata":{"execution":{"iopub.status.busy":"2024-01-05T21:31:09.594415Z","iopub.execute_input":"2024-01-05T21:31:09.59495Z","iopub.status.idle":"2024-01-05T21:31:09.615532Z","shell.execute_reply.started":"2024-01-05T21:31:09.594902Z","shell.execute_reply":"2024-01-05T21:31:09.614262Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"display(X_train.head())","metadata":{"execution":{"iopub.status.busy":"2024-01-05T21:31:09.617649Z","iopub.execute_input":"2024-01-05T21:31:09.618243Z","iopub.status.idle":"2024-01-05T21:31:09.642102Z","shell.execute_reply.started":"2024-01-05T21:31:09.618197Z","shell.execute_reply":"2024-01-05T21:31:09.640663Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Pipeline Feature Creation & Engineering","metadata":{}},{"cell_type":"code","source":"class SibSpParchTransformer(BaseEstimator, TransformerMixin):\n    def fit(self, X, y=None):\n        return self  \n\n    def transform(self, X):\n        X = X.copy()  \n        X['GroupSize'] = X['SibSp'] + X['Parch']\n        return X\n\n    \nclass CabinTransformer(BaseEstimator, TransformerMixin):\n    def fit(self, X, y=None):\n        return self\n\n    def transform(self, X):\n        X = X.copy()\n        X['Cab_Missing'] = np.where(X['Cabin'].isna(), 1, 0)\n        X['Deck'] = X['Cabin'].str.extract('([A-Za-z])', expand=False)\n        X['NumCabins'] = X['Cabin'].str.count(' ').add(1)\n        X['NumCabins'] = X['NumCabins'].fillna(X['NumCabins'].mode()[0]).astype(float)\n        X['FirstCabNum'] = X['Cabin'].str.extract('(\\d+)', expand=False).astype(float)\n\n        return X\n\n    \nclass TicketTransformer(BaseEstimator, TransformerMixin):\n    def __init__(self, ticketprefix_mapping=None):\n        if ticketprefix_mapping is None:\n            ticketprefix_mapping = {}\n        self.ticketprefix_mapping = ticketprefix_mapping\n\n    def fit(self, X, y=None):\n        # Preparing ticket count mapping for TicketGroupSize\n        self.ticket_counts_ = X['Ticket'].value_counts()\n        return self\n\n    def transform(self, X):\n        X = X.copy()  # Work on a copy\n        X['TicketPrefix'] = X['Ticket'].str.extract('([A-Za-z./]*[A-Za-z])', expand=False).str.replace('\\.', '').str.upper()\n        X['TicketNum'] = X['Ticket'].str.extract('(\\d+)$')\n        X['TicketGroupSize'] = X['Ticket'].map(self.ticket_counts_).astype(float)\n        X['TicketNumDigits'] = X['TicketNum'].str.len()\n        \n        # Apply mappings and fill missing values\n        X['TicketPrefix'] = X['TicketPrefix'].map(self.ticketprefix_mapping).fillna(X['TicketPrefix']).fillna('No_Prefix')\n        X['TicketNumDigits'] = X['TicketNumDigits'].fillna(0).astype(float)\n        X['TicketNum'] = X['TicketNum'].fillna(0).astype(float)\n        return X\n\n    \nclass NameTransformer(BaseEstimator, TransformerMixin):\n    def __init__(self, title_map=None):\n        if title_map is None:\n            title_map = {}\n        self.title_map = title_map\n\n    def fit(self, X, y=None):\n        # Preparing last name counts for LastNameRepeat\n        self.last_name_counts_ = X['Name'].str.extract('^([^,]+),', expand=False).value_counts()\n        return self\n\n    def transform(self, X):\n        X = X.copy()  # Work on a copy\n        X['Title'] = X['Name'].str.extract(' ([A-Za-z]+)\\.', expand=False)\n        X['LastName'] = X['Name'].str.extract('^([^,]+),', expand=False)\n        X['LastNameRepeat'] = X['LastName'].map(self.last_name_counts_).astype(float)\n        \n        # Apply title mappings and fill missing values\n        X['Title'] = X['Title'].map(self.title_map).fillna(X['Title'])\n        return X\n\n\nclass FareTransformer(BaseEstimator, TransformerMixin):\n    def __init__(self, n_bins=10, strategy='quantile'):\n        self.n_bins = n_bins\n        self.strategy = strategy\n    \n    def fit(self, X, y=None):\n        self.discretizer_ = KBinsDiscretizer(n_bins=self.n_bins, encode='ordinal', strategy=self.strategy)\n        self.discretizer_.fit(X[['Fare']])\n        return self\n\n    def transform(self, X):\n        X = X.copy()  # Work on a copy\n        X['FareQcut'] = self.discretizer_.transform(X[['Fare']])\n        return X\n\n    \nclass EmbarkedTransformer(BaseEstimator, TransformerMixin):\n    def __init__(self):\n        self.imputer = SimpleImputer(strategy='most_frequent')\n\n    def fit(self, X, y=None):\n        # Fit the imputer on the Embarked column\n        self.imputer.fit(X[['Embarked']])\n        return self\n\n    def transform(self, X):\n        X = X.copy()  \n        X['Embarked'] = self.imputer.transform(X[['Embarked']])\n        return X","metadata":{"execution":{"iopub.status.busy":"2024-01-05T21:31:09.644369Z","iopub.execute_input":"2024-01-05T21:31:09.644825Z","iopub.status.idle":"2024-01-05T21:31:09.678183Z","shell.execute_reply.started":"2024-01-05T21:31:09.644789Z","shell.execute_reply":"2024-01-05T21:31:09.676482Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class GroupRarePrefixesTransformer(BaseEstimator, TransformerMixin):\n    def __init__(self, column, cutoff=5):\n        self.column = column \n        self.cutoff = cutoff  \n    \n    def fit(self, X, y=None):\n        value_counts = X[self.column].value_counts()\n        self.to_group_ = value_counts[value_counts < self.cutoff].index\n        return self\n    \n    def transform(self, X):\n        X = X.copy()  \n        X[self.column] = X[self.column].where(~X[self.column].isin(self.to_group_), other='OTHER')\n        return X\n\n    \nclass CbAgeImputer(BaseEstimator, TransformerMixin):\n    def __init__(self):\n        # Initializing with default hyperparameters\n        # Adjust any CatBoost hyperparameters as needed\n        self.model = CatBoostRegressor(iterations=100,\n                                       learning_rate=0.05,\n                                       depth=6,\n                                       cat_features=['name__Title', 'remainder__Sex'],\n                                       loss_function='MAE',\n                                       random_seed=42,\n                                       verbose=False)\n\n    def fit(self, X, y=None):\n        # Ensure that X is a pandas DataFrame\n        if isinstance(X, pd.DataFrame):\n            # Define the feature columns as they appear after the previous transformations\n            feature_cols = ['remainder__Pclass', \n                            'fam__SibSp', \n                            'fam__Parch', \n                            'name__Title', \n                            'remainder__Sex']\n            # Define the target column\n            target_col = 'remainder__Age'\n            \n            # Training the model on non-missing age rows\n            self.model.fit(X.loc[X[target_col].notnull(), feature_cols], \n                           X.loc[X[target_col].notnull(), target_col])\n        return self\n\n    def transform(self, X):\n        # Check if model is fitted\n        if not hasattr(self, 'model'):\n            raise RuntimeError(\"You must train the classifier before predicting data!\")\n        \n        # Make a copy of the DataFrame to avoid changing the original data\n        X_transformed = X.copy()\n\n        # Define the feature columns and target column again\n        feature_cols = ['remainder__Pclass', \n                        'fam__SibSp', \n                        'fam__Parch', \n                        'name__Title', \n                        'remainder__Sex']\n        target_col = 'remainder__Age'\n\n        # Apply the model to impute missing values\n        missing_mask = X_transformed[target_col].isnull()\n        if missing_mask.any():\n            imputed_values = self.model.predict(X_transformed.loc[missing_mask, feature_cols])\n            X_transformed.loc[missing_mask, target_col] = imputed_values\n        \n        return X_transformed","metadata":{"execution":{"iopub.status.busy":"2024-01-05T21:31:09.680447Z","iopub.execute_input":"2024-01-05T21:31:09.680902Z","iopub.status.idle":"2024-01-05T21:31:09.698757Z","shell.execute_reply.started":"2024-01-05T21:31:09.680859Z","shell.execute_reply":"2024-01-05T21:31:09.697759Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Create Pipeline for Logistic Classifier","metadata":{}},{"cell_type":"code","source":"# Create the initial preprocessing pipeline\npre = ColumnTransformer(\n    transformers=[\n        ('fam', SibSpParchTransformer(), ['SibSp', 'Parch']),\n        ('cab', CabinTransformer(), ['Cabin']),\n        ('tick', TicketTransformer(), ['Ticket']),\n        ('name', NameTransformer(), ['Name']),\n        ('fare', FareTransformer(), ['Fare']),\n        ('embrk', EmbarkedTransformer(), ['Embarked']),\n    ],\n    remainder='passthrough'  \n)\n\n\n# Create the imputation and tagging pipeline\nimp = ColumnTransformer(\n    transformers=[\n        ('tikgrp', GroupRarePrefixesTransformer(column='tick__TicketPrefix', cutoff=5), make_column_selector(pattern='TicketPrefix')),\n        ('impcb', CbAgeImputer(), ['remainder__Pclass', 'fam__SibSp', 'fam__Parch', 'name__Title', 'remainder__Sex', 'remainder__Age']),\n        #('impcb', SimpleImputer(strategy='median'), ['remainder__Age']),\n        ('imp1', SimpleImputer(strategy='constant', fill_value='Unknown'), ['cab__Deck', 'cab__Cabin']),\n        #('imp1', SimpleImputer(strategy='most_frequent'), ['cab__Deck', 'cab__Cabin']),\n        ('imp2', SimpleImputer(strategy='constant', fill_value=0), ['cab__FirstCabNum']),\n        ('imp3', SimpleImputer(strategy='most_frequent'), ['cab__NumCabins']),\n    ],\n    remainder='passthrough'\n)\n\n\n# Create the column dropper pipeline which drops some original string columns with extreme cardinality\ndrop_logreg = ColumnTransformer(\n    transformers=[\n        ('drop1', 'drop', make_column_selector(pattern='remainder__name__Name')),\n        ('drop2', 'drop', make_column_selector(pattern='imp1__cab__Cabin')),\n        ('drop3', 'drop', make_column_selector(pattern='remainder__tick__Ticket')),\n        ('drop4', 'drop', make_column_selector(pattern='remainder__name__LastName')),\n        #('drop5', 'drop', make_column_selector(pattern='remainder__remainder__PassengerId')),\n    ],\n    remainder='passthrough'\n)\n\n\n# List of categorical columns to be transformed using ordinal encoder or one-hot encoder later on\ncat_columns_logreg = ['Sex', 'Embarked', 'Title']\nord_columns_logreg = ['Deck', 'TicketPrefix']\n\n# Create the Encoder pipeline\ncat_enc_logreg = ColumnTransformer(\n    transformers=[\n        ('enc1', OrdinalEncoder(handle_unknown='use_encoded_value', unknown_value=3000), make_column_selector(pattern=('|'.join(ord_columns_logreg)))),\n        ('enc2', OneHotEncoder(handle_unknown='ignore', sparse_output=False), make_column_selector(pattern=('|'.join(cat_columns_logreg)))),\n    ],\n    remainder='passthrough'\n)\n\n\n# Create Feature Engineering \npoly_logreg = PolynomialFeatures(degree=2, interaction_only=True, include_bias=False)\n\n# Create Distribution Transformer\ndist_logreg = QuantileTransformer(n_quantiles=50, output_distribution='normal')\n\n# Create Scaler\nscl_logreg = RobustScaler(unit_variance=False)\n\n# Create Feature Selection\nfeats_logreg = SelectFromModel(estimator=LogisticRegression(max_iter=1000), threshold=None, max_features=None)\n                   \n# Create Dimensionality Reduction: PCA\npca_logreg = PCA(n_components=0.95, whiten=False)\n\n# Create Classifier \nclf_logreg = LogisticRegression(penalty='l1', C=0.8, max_iter=700, solver='liblinear')\n\n# Create The Main PreProcessing Pipeline\npipe_logreg = Pipeline([\n    ('pre', pre),\n    ('imp', imp),\n    ('drop', drop_logreg),\n    ('enc', cat_enc_logreg),\n    ('poly', poly_logreg),\n    ('dist', dist_logreg),\n    ('scl', scl_logreg),\n    ('feats', feats_logreg),\n    ('pca', pca_logreg),\n    ('clf', clf_logreg)\n])\n\n# Grid options for 'poly' step\npoly_options_logreg = [\n    PolynomialFeatures(degree=1, interaction_only=True, include_bias=False),\n    #PolynomialFeatures(degree=2, interaction_only=False, include_bias=False),\n    #None  \n]\n\n# Grid options for 'dist' step\ndist_options_logreg = [\n    QuantileTransformer(n_quantiles=50, output_distribution='normal'),\n    #QuantileTransformer(n_quantiles=50, output_distribution='uniform'),\n    #PowerTransformer(method='yeo-johnson', standardize=True),\n    #None  \n]\n\n# Grid options for 'scl' step\nscl_options_logreg = [\n    RobustScaler(unit_variance=False),\n    #StandardScaler(),\n    #None  \n]\n\n# Grid options for 'feats' step\nfeats_options_logreg = [\n    SelectFromModel(estimator=LogisticRegression(max_iter=1000), threshold=None, max_features=None),\n    #SelectFromModel(estimator=KNeighborsClassifier(n_neighbors=5), threshold=None, max_features=None),\n    #SelectFromModel(estimator=RandomForestClassifier(n_estimators=200, max_depth=6, random_state=0), threshold=None, max_features=None),  \n    #SelectKBest(score_func=f_classif, k=14),\n    #VarianceThreshold(),\n    #None \n]\n\n# Grid options for 'pca' step\npca_options_logreg = [\n    #PCA(n_components=0.95, whiten=False),\n    #PCA(n_components=0.97, whiten=False),\n    None \n]\n\n# Grid options for 'clf' step \nclf_options_logreg = [\n    LogisticRegression(penalty='l1', C=0.8, max_iter=700, solver='liblinear'),\n    #LogisticRegression(penalty='l2', C=0.2, max_iter=700, solver='liblinear'),\n    #LogisticRegression(penalty='l1', C=0.9, max_iter=700, solver='liblinear'),\n]","metadata":{"execution":{"iopub.status.busy":"2024-01-05T21:31:09.700647Z","iopub.execute_input":"2024-01-05T21:31:09.701091Z","iopub.status.idle":"2024-01-05T21:31:09.729512Z","shell.execute_reply.started":"2024-01-05T21:31:09.701003Z","shell.execute_reply":"2024-01-05T21:31:09.728219Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Create K-Nearest Neighbor Classifier \n","metadata":{}},{"cell_type":"code","source":"# Create KNN model\nclf_knn = KNeighborsClassifier(n_neighbors=9)\n\n# Create pipeline structure for KNN which will borrow some steps from Logistic Regression due to similar pre-processsing and feature engineering requirements\npipe_knn = Pipeline([\n    ('pre', pre),\n    ('imp', imp),\n    ('drop', drop_logreg),\n    ('enc', cat_enc_logreg),\n    ('poly', poly_logreg),\n    ('dist', dist_logreg),\n    ('scl', scl_logreg),\n    ('feats', feats_logreg),\n    ('pca', pca_logreg),\n    ('clf', clf_knn)\n])\n\n# Grid options for 'poly' step\npoly_options_knn = [\n    PolynomialFeatures(degree=1, interaction_only=True, include_bias=False),\n    #PolynomialFeatures(degree=2, interaction_only=False, include_bias=False),\n    #None  \n]\n\n# Grid options for 'dist' step\ndist_options_knn = [\n    QuantileTransformer(n_quantiles=50, output_distribution='normal'),\n    #QuantileTransformer(n_quantiles=50, output_distribution='uniform'),\n    #PowerTransformer(method='yeo-johnson', standardize=True),\n    #None  \n]\n\n# Grid options for 'scl' step\nscl_options_knn = [\n    RobustScaler(unit_variance=False),\n    #StandardScaler(),\n    #None  \n]\n\n# Grid options for 'feats' step\nfeats_options_knn = [\n    #SelectFromModel(estimator=LogisticRegression(max_iter=1000), threshold=None, max_features=None),\n    #SelectFromModel(estimator=KNeighborsClassifier(n_neighbors=5), threshold=None, max_features=None),\n    #SelectFromModel(estimator=RandomForestClassifier(n_estimators=200, max_depth=6, random_state=0), threshold=None, max_features=None),  \n    #SelectKBest(score_func=f_classif, k=14),\n    VarianceThreshold(),\n    #None \n]\n\n# Grid options for 'pca' step\npca_options_knn = [\n    #PCA(n_components=0.95, whiten=False),\n    #PCA(n_components=0.97, whiten=False),\n    None \n]\n\n# Grid options for 'clf' step \nclf_options_knn = [\n    KNeighborsClassifier(n_neighbors=9),\n    #KNeighborsClassifier(n_neighbors=10),\n]","metadata":{"execution":{"iopub.status.busy":"2024-01-05T21:31:09.735262Z","iopub.execute_input":"2024-01-05T21:31:09.736376Z","iopub.status.idle":"2024-01-05T21:31:09.747875Z","shell.execute_reply.started":"2024-01-05T21:31:09.736335Z","shell.execute_reply":"2024-01-05T21:31:09.746638Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Create Naive Bayes Classifier","metadata":{}},{"cell_type":"code","source":"# Create GNB model\nclf_gnb = GaussianNB()\n\n# Create pipeline structure for KNN which will borrow some steps from Logistic Regression due to similar pre-processsing and feature engineering requirements\npipe_gnb = Pipeline([\n    ('pre', pre),\n    ('imp', imp),\n    ('drop', drop_logreg),\n    ('enc', cat_enc_logreg),\n    ('poly', poly_logreg),\n    ('dist', dist_logreg),\n    ('scl', scl_logreg),\n    ('feats', feats_logreg),\n    ('pca', pca_logreg),\n    ('clf', clf_gnb)\n])\n\n# Grid options for 'poly' step\npoly_options_gnb = [\n    PolynomialFeatures(degree=1, interaction_only=True, include_bias=False),\n    #PolynomialFeatures(degree=2, interaction_only=False, include_bias=False),\n    #None  \n]\n\n# Grid options for 'dist' step\ndist_options_gnb = [\n    QuantileTransformer(n_quantiles=50, output_distribution='normal'),\n    #QuantileTransformer(n_quantiles=50, output_distribution='uniform'),\n    #PowerTransformer(method='yeo-johnson', standardize=True),\n    #None  \n]\n\n# Grid options for 'scl' step\nscl_options_gnb = [\n    RobustScaler(unit_variance=False),\n    #StandardScaler(),\n    #None  \n]\n\n# Grid options for 'feats' step\nfeats_options_gnb = [\n    #SelectFromModel(estimator=LogisticRegression(max_iter=1000), threshold=None, max_features=None),\n    #SelectFromModel(estimator=KNeighborsClassifier(n_neighbors=5), threshold=None, max_features=None),\n    #SelectFromModel(estimator=RandomForestClassifier(n_estimators=200, max_depth=6, random_state=0), threshold=None, max_features=None),  \n    #SelectKBest(score_func=f_classif, k=14),\n    VarianceThreshold(),\n    #None \n]\n\n# Grid options for 'pca' step\npca_options_gnb = [\n    #PCA(n_components=0.95, whiten=False),\n    #PCA(n_components=0.97, whiten=False),\n    None \n]\n\n# Grid options for 'clf' step \nclf_options_gnb = [\n    GaussianNB()\n]","metadata":{"execution":{"iopub.status.busy":"2024-01-05T21:31:09.749883Z","iopub.execute_input":"2024-01-05T21:31:09.75031Z","iopub.status.idle":"2024-01-05T21:31:09.77658Z","shell.execute_reply.started":"2024-01-05T21:31:09.750274Z","shell.execute_reply":"2024-01-05T21:31:09.775069Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Create SVC Classifier","metadata":{}},{"cell_type":"code","source":"# Create SVC model with initial hyperparameters\nclf_svc = SVC(kernel='rbf', C=8, gamma='auto', probability=True, class_weight=None) \n\n# Create pipeline structure for KNN which will borrow some steps from Logistic Regression due to similar pre-processsing and feature engineering requirements\npipe_svc = Pipeline([\n    ('pre', pre),\n    ('imp', imp),\n    ('drop', drop_logreg),\n    ('enc', cat_enc_logreg),\n    ('poly', poly_logreg),\n    ('dist', dist_logreg),\n    ('scl', scl_logreg),\n    ('feats', feats_logreg),\n    ('pca', pca_logreg),\n    ('clf', clf_svc)\n])\n\n# Grid options for 'poly' step\npoly_options_svc = [\n    PolynomialFeatures(degree=1, interaction_only=True, include_bias=False),\n    #PolynomialFeatures(degree=2, interaction_only=False, include_bias=False),\n    #None  \n]\n\n# Grid options for 'dist' step\ndist_options_svc = [\n    QuantileTransformer(n_quantiles=50, output_distribution='normal'),\n    #QuantileTransformer(n_quantiles=50, output_distribution='uniform'),\n    #PowerTransformer(method='yeo-johnson', standardize=True),\n    #None  \n]\n\n# Grid options for 'scl' step\nscl_options_svc = [\n    RobustScaler(unit_variance=False),\n    #StandardScaler(),\n    #None  \n]\n\n# Grid options for 'feats' step\nfeats_options_svc = [\n    #SelectFromModel(estimator=LogisticRegression(max_iter=1000), threshold=None, max_features=None),\n    #SelectFromModel(estimator=KNeighborsClassifier(n_neighbors=5), threshold=None, max_features=None),\n    #SelectFromModel(estimator=RandomForestClassifier(n_estimators=200, max_depth=6, random_state=0), threshold=None, max_features=None),  \n    #SelectKBest(score_func=f_classif, k=14),\n    VarianceThreshold(),\n    #None \n]\n\n# Grid options for 'pca' step\npca_options_svc = [\n    #PCA(n_components=0.95, whiten=False),\n    #PCA(n_components=0.97, whiten=False),\n    None \n]\n\n# Grid options for 'clf' step \nclf_options_svc = [\n    SVC(kernel='rbf', C=8, gamma='auto', probability=True, class_weight=None),\n]","metadata":{"execution":{"iopub.status.busy":"2024-01-05T21:31:09.779136Z","iopub.execute_input":"2024-01-05T21:31:09.779703Z","iopub.status.idle":"2024-01-05T21:31:09.794996Z","shell.execute_reply.started":"2024-01-05T21:31:09.779651Z","shell.execute_reply":"2024-01-05T21:31:09.793883Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Create SGD Classifier","metadata":{}},{"cell_type":"code","source":"# Create SGD model with initial hyperparameters\nclf_sgd = SGDClassifier(loss='log_loss', penalty='elasticnet', alpha=0.07, l1_ratio=0.10, max_iter=1000, random_state=0)\n\n# Create pipeline structure for KNN which will borrow some steps from Logistic Regression due to similar pre-processsing and feature engineering requirements\npipe_sgd = Pipeline([\n    ('pre', pre),\n    ('imp', imp),\n    ('drop', drop_logreg),\n    ('enc', cat_enc_logreg),\n    ('poly', poly_logreg),\n    ('dist', dist_logreg),\n    ('scl', scl_logreg),\n    ('feats', feats_logreg),\n    ('pca', pca_logreg),\n    ('clf', clf_sgd)\n])\n\n# Grid options for 'poly' step\npoly_options_sgd = [\n    PolynomialFeatures(degree=1, interaction_only=True, include_bias=False),\n    #PolynomialFeatures(degree=2, interaction_only=False, include_bias=False),\n    #None  \n]\n\n# Grid options for 'dist' step\ndist_options_sgd = [\n    QuantileTransformer(n_quantiles=50, output_distribution='normal'),\n    #QuantileTransformer(n_quantiles=50, output_distribution='uniform'),\n    #PowerTransformer(method='yeo-johnson', standardize=True),\n    #None  \n]\n\n# Grid options for 'scl' step\nscl_options_sgd = [\n    RobustScaler(unit_variance=False),\n    #StandardScaler(),\n    #None  \n]\n\n# Grid options for 'feats' step\nfeats_options_sgd = [\n    #SelectFromModel(estimator=LogisticRegression(max_iter=1000), threshold=None, max_features=None),\n    #SelectFromModel(estimator=KNeighborsClassifier(n_neighbors=5), threshold=None, max_features=None),\n    #SelectFromModel(estimator=RandomForestClassifier(n_estimators=200, max_depth=6, random_state=0), threshold=None, max_features=None),  \n    #SelectKBest(score_func=f_classif, k=14),\n    VarianceThreshold(),\n    #None \n]\n\n# Grid options for 'pca' step\npca_options_sgd = [\n    #PCA(n_components=0.95, whiten=False),\n    #PCA(n_components=0.97, whiten=False),\n    None \n]\n\n# Grid options for 'clf' step \nclf_options_sgd = [\n    SGDClassifier(loss='log_loss', penalty='elasticnet', alpha=0.07, l1_ratio=0.10, max_iter=1000, random_state=0),\n    #SGDClassifier(loss='modified_huber', penalty='l2', alpha=0.0001, l1_ratio=0.15, max_iter=1000),\n    \n]","metadata":{"execution":{"iopub.status.busy":"2024-01-05T21:31:09.797158Z","iopub.execute_input":"2024-01-05T21:31:09.797652Z","iopub.status.idle":"2024-01-05T21:31:09.814802Z","shell.execute_reply.started":"2024-01-05T21:31:09.797594Z","shell.execute_reply":"2024-01-05T21:31:09.813643Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Create Random Forest Classifier","metadata":{}},{"cell_type":"code","source":"# Create Classifier\nclf_rf = RandomForestClassifier(n_estimators=100, max_depth=5)\n\n# Create pipeline \npipe_rf = Pipeline([\n    ('pre', pre),\n    ('imp', imp),\n    ('drop', drop_logreg),\n    ('enc', cat_enc_logreg),\n    ('poly', poly_logreg),\n    ('dist', dist_logreg),\n    ('scl', scl_logreg),\n    ('feats', feats_logreg),\n    ('pca', pca_logreg),\n    ('clf', clf_rf)\n])\n\n# Grid options for 'poly' step\npoly_options_rf = [\n    PolynomialFeatures(degree=1, interaction_only=True, include_bias=False),\n    #PolynomialFeatures(degree=2, interaction_only=False, include_bias=False),\n    #None  \n]\n\n# Grid options for 'dist' step\ndist_options_rf = [\n    QuantileTransformer(n_quantiles=50, output_distribution='normal'),\n    #QuantileTransformer(n_quantiles=50, output_distribution='uniform'),\n    #PowerTransformer(method='yeo-johnson', standardize=True),\n    #None  \n]\n\n# Grid options for 'scl' step\nscl_options_rf = [\n    RobustScaler(unit_variance=False),\n    #StandardScaler(),\n    #None  \n]\n\n# Grid options for 'feats' step\nfeats_options_rf = [\n    #SelectFromModel(estimator=RandomForestClassifier(n_estimators=100, max_depth=4), threshold=None, max_features=None),  \n    #SelectFromModel(estimator=LogisticRegression(max_iter=1000), threshold=None, max_features=None),\n    #SelectKBest(score_func=f_classif, k=14),\n    VarianceThreshold(),\n    #None \n]\n\n# Grid options for 'pca' step\npca_options_rf = [\n    #PCA(n_components=0.95, whiten=False),\n    #PCA(n_components=0.97, whiten=False),\n    None \n]\n\n# Grid options for 'clf' step \nclf_options_rf = [\n    RandomForestClassifier(n_estimators=100, max_depth=5),\n]","metadata":{"execution":{"iopub.status.busy":"2024-01-05T21:31:09.817247Z","iopub.execute_input":"2024-01-05T21:31:09.817772Z","iopub.status.idle":"2024-01-05T21:31:09.833944Z","shell.execute_reply.started":"2024-01-05T21:31:09.817706Z","shell.execute_reply":"2024-01-05T21:31:09.832552Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Create XGBoost Classifier","metadata":{}},{"cell_type":"code","source":"## Create Classifier\nclf_xgb = xgb.XGBClassifier(n_estimators=100, learning_rate=0.06,max_depth=3,gamma=0.01,subsample=1)\n\n# Create pipeline \npipe_xgb = Pipeline([\n    ('pre', pre),\n    ('imp', imp),\n    ('drop', drop_logreg),\n    ('enc', cat_enc_logreg),\n    ('poly', poly_logreg),\n    ('dist', dist_logreg),\n    ('scl', scl_logreg),\n    ('feats', feats_logreg),\n    ('pca', pca_logreg),\n    ('clf', clf_xgb)\n])\n\n# Grid options for 'poly' step\npoly_options_xgb = [\n    PolynomialFeatures(degree=1, interaction_only=True, include_bias=False),\n    #PolynomialFeatures(degree=2, interaction_only=False, include_bias=False),\n    #None  \n]\n\n# Grid options for 'dist' step\ndist_options_xgb = [\n    QuantileTransformer(n_quantiles=50, output_distribution='normal'),\n    #QuantileTransformer(n_quantiles=50, output_distribution='uniform'),\n    #PowerTransformer(method='yeo-johnson', standardize=True),\n    #None  \n]\n\n# Grid options for 'scl' step\nscl_options_xgb = [\n    RobustScaler(unit_variance=False),\n    #StandardScaler(),\n    #None  \n]\n\n# Grid options for 'feats' step\nfeats_options_xgb = [\n    SelectFromModel(estimator=xgb.XGBClassifier(n_estimators=200, learning_rate=0.05,max_depth=3,gamma=0,subsample=1), threshold=None, max_features=None),  \n    #SelectFromModel(estimator=LogisticRegression(max_iter=1000), threshold=None, max_features=None),\n    #SelectKBest(score_func=f_classif, k=14),\n    #VarianceThreshold(),\n    #None \n]\n\n# Grid options for 'pca' step\npca_options_xgb = [\n    #PCA(n_components=0.95, whiten=False),\n    #PCA(n_components=0.97, whiten=False),\n    None \n]\n\n# Grid options for 'clf' step \nclf_options_xgb = [\n    xgb.XGBClassifier(n_estimators=100, learning_rate=0.06,max_depth=3,gamma=0.01,subsample=1),\n    #xgb.XGBClassifier()\n]","metadata":{"execution":{"iopub.status.busy":"2024-01-05T21:31:09.835453Z","iopub.execute_input":"2024-01-05T21:31:09.836006Z","iopub.status.idle":"2024-01-05T21:31:09.850303Z","shell.execute_reply.started":"2024-01-05T21:31:09.835968Z","shell.execute_reply":"2024-01-05T21:31:09.84916Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Create Light GBM Classifier","metadata":{}},{"cell_type":"code","source":"## Create Classifier\nclf_lgb = lgb.LGBMClassifier(n_estimators=300, num_leaves=31, max_depth=3, learning_rate=0.04)\n\n# Create pipeline \npipe_lgb = Pipeline([\n    ('pre', pre),\n    ('imp', imp),\n    ('drop', drop_logreg),\n    ('enc', cat_enc_logreg),\n    ('poly', poly_logreg),\n    ('dist', dist_logreg),\n    ('scl', scl_logreg),\n    ('feats', feats_logreg),\n    ('pca', pca_logreg),\n    ('clf', clf_lgb)\n])\n\n# Grid options for 'poly' step\npoly_options_lgb = [\n    PolynomialFeatures(degree=1, interaction_only=True, include_bias=False),\n    #PolynomialFeatures(degree=2, interaction_only=False, include_bias=False),\n    #None  \n]\n\n# Grid options for 'dist' step\ndist_options_lgb = [\n    QuantileTransformer(n_quantiles=50, output_distribution='normal'),\n    #QuantileTransformer(n_quantiles=50, output_distribution='uniform'),\n    #PowerTransformer(method='yeo-johnson', standardize=True),\n    #None  \n]\n\n# Grid options for 'scl' step\nscl_options_lgb = [\n    RobustScaler(unit_variance=False),\n    #StandardScaler(),\n    #None  \n]\n\n# Grid options for 'feats' step\nfeats_options_lgb = [\n    #SelectFromModel(estimator=lgb.LGBMClassifier(n_estimators=200, num_leaves=31, max_depth=4, learning_rate=0.03), threshold=None, max_features=None),  \n    #SelectFromModel(estimator=LogisticRegression(max_iter=1000), threshold=None, max_features=None),\n    #SelectKBest(score_func=f_classif, k=14),\n    VarianceThreshold(),\n    #None \n]\n\n# Grid options for 'pca' step\npca_options_lgb = [\n    #PCA(n_components=0.95, whiten=False),\n    #PCA(n_components=0.97, whiten=False),\n    None \n]\n\n# Grid options for 'clf' step \nclf_options_lgb = [\n    lgb.LGBMClassifier(n_estimators=300, num_leaves=31, max_depth=3, learning_rate=0.04),\n]","metadata":{"execution":{"iopub.status.busy":"2024-01-05T21:31:09.85199Z","iopub.execute_input":"2024-01-05T21:31:09.852556Z","iopub.status.idle":"2024-01-05T21:31:09.868066Z","shell.execute_reply.started":"2024-01-05T21:31:09.852518Z","shell.execute_reply":"2024-01-05T21:31:09.866979Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Voting Classifier Ensemble","metadata":{}},{"cell_type":"code","source":"# Create voting classifier\nvoting_clf = VotingClassifier(\n    estimators=[\n        ('logreg', pipe_logreg), \n        ('knn', pipe_knn), \n        ('gnb', pipe_gnb),\n        ('svc', pipe_svc),\n        ('sgd', pipe_sgd),\n        ('rf', pipe_rf),\n        ('xgb', pipe_xgb),\n        ('lgb', pipe_lgb),\n    ], voting='soft', verbose=False, flatten_transform=True)  \n\nparam_grid = {\n    # Voting Classifier Parameters\n    'weights': [\n        [1, 1, 1, 1, 1, 1, 1, 1],\n        #[1, 0, 1, 3, 1, 3, 3, 3],\n        #[0, 0, 0, 0, 0, 0, 0, 0],\n        #[8, 8, 7, 8, 7, 8, 7, 8], \n    ],\n    # Logistic Regression Pipeline Options\n    'logreg__poly': poly_options_logreg,\n    'logreg__dist': dist_options_logreg,\n    'logreg__scl': scl_options_logreg,\n    'logreg__feats': feats_options_logreg,\n    'logreg__pca': pca_options_logreg,\n    'logreg__clf': clf_options_logreg,\n    \n    # KNN Pipeline Options\n    'knn__poly': poly_options_knn,\n    'knn__dist': dist_options_knn,\n    'knn__scl': scl_options_knn,\n    'knn__feats': feats_options_knn,\n    'knn__pca': pca_options_knn,\n    'knn__clf': clf_options_knn,\n    \n    # GNB Pipeline Options\n    'gnb__poly': poly_options_knn,\n    'gnb__dist': dist_options_knn,\n    'gnb__scl': scl_options_knn,\n    'gnb__feats': feats_options_knn,\n    'gnb__pca': pca_options_knn,\n    'gnb__clf': clf_options_knn,\n    \n    # SVC Pipeline Options\n    'svc__poly': poly_options_svc,\n    'svc__dist': dist_options_svc,\n    'svc__scl': scl_options_svc,\n    'svc__feats': feats_options_svc,\n    'svc__pca': pca_options_svc,\n    'svc__clf': clf_options_svc,\n    \n    # SGD Pipeline Options\n    'sgd__poly': poly_options_sgd,\n    'sgd__dist': dist_options_sgd,\n    'sgd__scl': scl_options_sgd,\n    'sgd__feats': feats_options_sgd,\n    'sgd__pca': pca_options_sgd,\n    'sgd__clf': clf_options_sgd,\n\n     # RF Pipeline Options\n    'rf__poly': poly_options_rf,\n    'rf__dist': dist_options_rf,\n    'rf__scl': scl_options_rf,\n    'rf__feats': feats_options_rf,\n    'rf__pca': pca_options_rf,\n    'rf__clf': clf_options_rf,\n    \n    # XGB Pipeline Options\n    'xgb__poly': poly_options_xgb,\n    'xgb__dist': dist_options_xgb,\n    'xgb__scl': scl_options_xgb,\n    'xgb__feats': feats_options_xgb,\n    'xgb__pca': pca_options_xgb,\n    'xgb__clf': clf_options_xgb,\n    \n    # LGB Pipeline Options\n    'lgb__poly': poly_options_lgb,\n    'lgb__dist': dist_options_lgb,\n    'lgb__scl': scl_options_lgb,\n    'lgb__feats': feats_options_lgb,\n    'lgb__pca': pca_options_lgb,\n    'lgb__clf': clf_options_lgb,\n}\n\n# Set up the GridSearchCV to find the best parameters (including weights)\nstratified_kfold = StratifiedKFold(n_splits=7, shuffle=True, random_state=42)\ngrid_search = GridSearchCV(estimator=voting_clf, param_grid=param_grid, cv=stratified_kfold, n_jobs=-1, scoring='accuracy', return_train_score=True, verbose=False)\n\n# Execute grid search\ngrid_search.fit(X_train, y_train)\n\n# Print grid search key results \nprint(\"Best weights: \", grid_search.best_params_['weights'])\nprint(\"Best parameters: \", grid_search.best_params_)\nprint(\"Mean test accuracy scores for each parameter combination: \", grid_search.cv_results_['mean_test_score'])\nprint(\"Mean training accuracy scores for each parameter combination: \", grid_search.cv_results_['mean_train_score'])\nprint(\"Best cross-validation accuracy: \", grid_search.best_score_)\n\n\n# Turn off warnings\nwarnings.filterwarnings('ignore')","metadata":{"execution":{"iopub.status.busy":"2024-01-05T21:31:09.870171Z","iopub.execute_input":"2024-01-05T21:31:09.870632Z","iopub.status.idle":"2024-01-05T21:31:50.290965Z","shell.execute_reply.started":"2024-01-05T21:31:09.870595Z","shell.execute_reply":"2024-01-05T21:31:50.289853Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Report Metrics","metadata":{}},{"cell_type":"code","source":"# Use the best estimator from the grid search to make predictions on the test set\nbest_estimator = grid_search.best_estimator_\ny_pred = best_estimator.predict(X_hold)\n\n# Calculate accuracy\naccuracy = accuracy_score(y_hold, y_pred)\nprint(\"Accuracy:\", accuracy)\n\n# Generate the confusion matrix\nconf_matrix = confusion_matrix(y_hold, y_pred)\nplt.figure(figsize=(4, 2))  # Size of the figure\nax = sns.heatmap(conf_matrix, annot=True, fmt=\".1f\", cmap=\"mako\", cbar=False)\nplt.title('Confusion Matrix', color='black')\nplt.xlabel('Predicted Labels', color='black')\nplt.ylabel('True Labels', color='black')\nplt.tick_params(colors='white', which='both')  # Tick colors\nax.xaxis.label.set_color('black')  # X label color\nax.yaxis.label.set_color('black')  # Y label color\nplt.show()\n\n# Generate the classification report\nclass_report = classification_report(y_hold, y_pred)\nprint(\"Classification Report:\")\nprint(class_report)\n\n# Display the best estimator from grid search that was used for predictions and classification reporting\nprint(\"Best Estimator:\", best_estimator)","metadata":{"execution":{"iopub.status.busy":"2024-01-05T21:31:50.29242Z","iopub.execute_input":"2024-01-05T21:31:50.293504Z","iopub.status.idle":"2024-01-05T21:31:52.802166Z","shell.execute_reply.started":"2024-01-05T21:31:50.293466Z","shell.execute_reply":"2024-01-05T21:31:52.799576Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Predict On New/Test Data & Generate CSV File","metadata":{}},{"cell_type":"code","source":"# Read test dataset from folders:\ndf_test = pd.read_csv('/kaggle/input/titanic/test.csv')\n\n# No imputation was built in pipelines for Fare column as it had no NaN values in training dataset\ndf_test = (df_test\n          .assign(\n              Fare=lambda df_: df_.Fare.fillna(df_.Fare.mean()))\n          )\n\ndisplay(df_test.info())","metadata":{"execution":{"iopub.status.busy":"2024-01-05T21:31:52.804513Z","iopub.execute_input":"2024-01-05T21:31:52.805025Z","iopub.status.idle":"2024-01-05T21:31:52.849339Z","shell.execute_reply.started":"2024-01-05T21:31:52.804981Z","shell.execute_reply":"2024-01-05T21:31:52.84766Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Predict\npredictions = grid_search.predict(df_test)\n\n# Format results and print to CSV\ndf_pred = pd.DataFrame(predictions, columns=['Survived'])\ndf_pred.index = df_test.index\ndf_test_with_pred = (pd\n                     .concat([df_test, df_pred], axis=1)\n                     .loc[:, ['PassengerId', 'Survived']]\n                    )\ndisplay(df_test_with_pred)\n\ndf_test_with_pred.to_csv('titanic_pred_9.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2024-01-05T21:31:52.851721Z","iopub.execute_input":"2024-01-05T21:31:52.852379Z","iopub.status.idle":"2024-01-05T21:31:53.981901Z","shell.execute_reply.started":"2024-01-05T21:31:52.852334Z","shell.execute_reply":"2024-01-05T21:31:53.980076Z"},"trusted":true},"execution_count":null,"outputs":[]}]}